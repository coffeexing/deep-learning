{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Two Layer Net"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1bf7964b4b339707"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(os.pardir)\n",
    "from Data.common.functions import *\n",
    "from Data.common.gradient import numerical_gradient\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, \n",
    "                 weight_init_std=0.01):\n",
    "        # Initialization\n",
    "        self.parms = {}\n",
    "        self.parms['W1'] = (weight_init_std * \n",
    "                            np.random.randn(input_size, hidden_size))\n",
    "        self.parms['b1'] = np.zeros(hidden_size)\n",
    "        self.parms['W2'] = (weight_init_std *\n",
    "                            np.random.randn(hidden_size, output_size))\n",
    "        self.parms['b2'] = np.zeros(output_size)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.parms['W1'], self.parms['W2']\n",
    "        b1, b2 = self.parms['b1'], self.parms['b2']\n",
    "        \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    # x:input data, t:supervised data\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    # x:input data, t:supervised data\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.parms['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.parms['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.parms['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.parms['b2'])\n",
    "        \n",
    "        return grads"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-28T02:34:52.732710400Z",
     "start_time": "2025-08-28T02:34:52.181362200Z"
    }
   },
   "id": "59bda6a9ab3b4f43"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Example"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cbc05ce6c3cacc45"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "(10,)"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = TwoLayerNet(input_size=784, hidden_size=100, output_size=10)\n",
    "net.parms['W1'].shape  # (784, 100)\n",
    "net.parms['b1'].shape  # (100, )\n",
    "net.parms['W2'].shape  # (100, 10)\n",
    "net.parms['b2'].shape  # (10, )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-28T02:34:53.224621400Z",
     "start_time": "2025-08-28T02:34:52.734710700Z"
    }
   },
   "id": "c55ab6b5ab46e82f"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.93355971  1.81330377  1.10522476 ...  1.300684   -0.06946814\n",
      "   0.00790263]\n",
      " [-0.68416527 -0.25575427  1.62561394 ... -0.0815943   0.15976661\n",
      "  -1.16795041]\n",
      " [-0.02522362  0.22759012  0.03006273 ...  0.95299973 -0.30842773\n",
      "  -0.13670869]\n",
      " ...\n",
      " [-0.33497465 -0.57036121 -0.82067943 ... -1.11202925  0.8079055\n",
      "  -0.75582429]\n",
      " [ 0.04287831  0.17515853 -1.20598381 ...  0.98240858  0.53983976\n",
      "   2.04316759]\n",
      " [ 0.13823927 -1.03725688  0.70451859 ...  1.35503249 -1.50235947\n",
      "   0.56810008]] [[0.10183856 0.10047881 0.09771114 0.10763618 0.10954496 0.09982238\n",
      "  0.09048761 0.09017318 0.11183786 0.09046932]\n",
      " [0.10078394 0.10049247 0.09698397 0.10969162 0.10829953 0.09916909\n",
      "  0.09004153 0.09087258 0.11161902 0.09204625]\n",
      " [0.10193418 0.10042756 0.09753037 0.10842869 0.10759443 0.09942182\n",
      "  0.09091496 0.09114721 0.11086092 0.09173986]\n",
      " [0.10003145 0.10074041 0.09860755 0.10740731 0.10846876 0.10010079\n",
      "  0.08987128 0.09034782 0.11239723 0.09202739]\n",
      " [0.0991108  0.10086383 0.09830252 0.10773203 0.10885159 0.09951793\n",
      "  0.08962423 0.09178359 0.11173981 0.09247368]\n",
      " [0.10060033 0.1011499  0.09738088 0.10885506 0.10789919 0.09918241\n",
      "  0.09050014 0.09080402 0.11056332 0.09306475]\n",
      " [0.10092397 0.10179879 0.09790317 0.10905443 0.10807009 0.09926789\n",
      "  0.0903626  0.09096876 0.10961166 0.09203863]\n",
      " [0.10088341 0.10222232 0.09781964 0.10824955 0.10762643 0.09966506\n",
      "  0.09007376 0.0904902  0.11102116 0.09194848]\n",
      " [0.09992265 0.10083762 0.09646683 0.10813209 0.10763178 0.10022588\n",
      "  0.09082901 0.0914475  0.1123633  0.09214334]\n",
      " [0.10091873 0.10090314 0.09810077 0.10886005 0.10805491 0.09985359\n",
      "  0.09082254 0.09040363 0.11040728 0.09167536]\n",
      " [0.10093419 0.09981318 0.09763056 0.11015111 0.10795422 0.09934525\n",
      "  0.09093042 0.09080931 0.11037824 0.09205353]\n",
      " [0.10231412 0.10114921 0.09746721 0.10784355 0.10794507 0.09951739\n",
      "  0.0901941  0.0906016  0.11056674 0.09240099]\n",
      " [0.10056979 0.09980839 0.09854446 0.10893599 0.10659452 0.09940941\n",
      "  0.08990103 0.09187908 0.11192487 0.09243246]\n",
      " [0.1007563  0.10097584 0.09815518 0.10719841 0.10736224 0.09994255\n",
      "  0.09062803 0.09158191 0.11031328 0.09308627]\n",
      " [0.09977702 0.10030654 0.09737589 0.10790947 0.10953382 0.10036674\n",
      "  0.0900791  0.09041296 0.1121919  0.09204656]\n",
      " [0.09965501 0.10010513 0.09714205 0.10928632 0.10812848 0.10080267\n",
      "  0.09041708 0.09080673 0.11195534 0.09170119]\n",
      " [0.1017668  0.09997138 0.09898899 0.10890504 0.10739759 0.09916706\n",
      "  0.08953007 0.09040855 0.11069486 0.09316966]\n",
      " [0.10069947 0.10193182 0.09864942 0.1089051  0.1081347  0.09874989\n",
      "  0.09085029 0.08997893 0.10975782 0.09234256]\n",
      " [0.10059918 0.10134937 0.09740713 0.10952128 0.10749438 0.09902042\n",
      "  0.09115991 0.09123636 0.10967906 0.09253291]\n",
      " [0.09947799 0.10157687 0.09759739 0.10844521 0.10762802 0.09865249\n",
      "  0.0907144  0.09081742 0.11112657 0.09396363]\n",
      " [0.09886601 0.10089728 0.09864982 0.1092896  0.10761767 0.09907396\n",
      "  0.09100072 0.09218173 0.10955523 0.09286797]\n",
      " [0.09966187 0.10071843 0.09810196 0.10847725 0.10766992 0.10013501\n",
      "  0.09020281 0.09045917 0.11134723 0.09322634]\n",
      " [0.10000528 0.1012395  0.09775998 0.10826441 0.10921161 0.09935272\n",
      "  0.08915579 0.09039796 0.11189182 0.09272093]\n",
      " [0.10082728 0.10048925 0.09793628 0.10930273 0.10825462 0.09928233\n",
      "  0.09023653 0.09195092 0.1113182  0.09040186]\n",
      " [0.10025081 0.09955296 0.09700696 0.10823558 0.10964082 0.10038386\n",
      "  0.09069808 0.09203133 0.11007137 0.09212823]\n",
      " [0.10072238 0.10077519 0.09758801 0.10992751 0.10742116 0.09838775\n",
      "  0.09017808 0.09116406 0.11060044 0.09323542]\n",
      " [0.10236255 0.09994915 0.09623927 0.10796892 0.1071176  0.09912214\n",
      "  0.090797   0.09222417 0.11106684 0.09315236]\n",
      " [0.09995731 0.10087333 0.09933985 0.10855633 0.10697553 0.09992077\n",
      "  0.08909254 0.09060477 0.1112416  0.09343795]\n",
      " [0.10282866 0.10171658 0.09700392 0.10845538 0.10704991 0.09888803\n",
      "  0.09013749 0.09087822 0.11049168 0.09255012]\n",
      " [0.10035148 0.10029513 0.09734622 0.10907071 0.10765534 0.10048913\n",
      "  0.09103593 0.09190626 0.10948774 0.09236205]\n",
      " [0.10121613 0.1012011  0.0975547  0.10795746 0.10867329 0.0990946\n",
      "  0.09038099 0.09133273 0.11135855 0.09123044]\n",
      " [0.10087025 0.09974601 0.09903149 0.10831037 0.10773324 0.09963972\n",
      "  0.09097638 0.09137608 0.11053787 0.09177859]\n",
      " [0.1030385  0.10057894 0.09671166 0.10843192 0.10873655 0.09903163\n",
      "  0.0905259  0.09067323 0.11105227 0.09121939]\n",
      " [0.10060079 0.10199697 0.09705118 0.10829623 0.10728455 0.09884117\n",
      "  0.08996692 0.09061604 0.11089872 0.09444743]\n",
      " [0.10096277 0.09995127 0.0971401  0.10828084 0.1078865  0.10014032\n",
      "  0.09062009 0.09165315 0.11064845 0.09271652]\n",
      " [0.10049806 0.100478   0.09720206 0.10969525 0.1083094  0.09859094\n",
      "  0.0908075  0.09127336 0.10995554 0.09318988]\n",
      " [0.10101584 0.10035413 0.09817019 0.10818583 0.1078043  0.09951635\n",
      "  0.09015308 0.09131212 0.11163206 0.09185611]\n",
      " [0.09928241 0.10164149 0.09799549 0.10831456 0.1084869  0.10026868\n",
      "  0.09062857 0.09014404 0.11159621 0.09164165]\n",
      " [0.0999797  0.10091597 0.09749479 0.10816548 0.10866517 0.09955443\n",
      "  0.09021412 0.09072952 0.11255576 0.09172505]\n",
      " [0.10020249 0.10062057 0.09636999 0.10957409 0.10801849 0.09902922\n",
      "  0.09074045 0.09167516 0.11193701 0.09183253]\n",
      " [0.10113658 0.10128157 0.09745155 0.10856423 0.10787164 0.10057678\n",
      "  0.08972782 0.0897813  0.1110478  0.09256073]\n",
      " [0.10228513 0.10102055 0.09747266 0.10779443 0.10748479 0.09931261\n",
      "  0.09054206 0.09121095 0.11017465 0.09270218]\n",
      " [0.10009587 0.10054807 0.0976967  0.1090331  0.10822838 0.09866405\n",
      "  0.09037212 0.09242995 0.10992504 0.09300673]\n",
      " [0.10024029 0.10083864 0.09793733 0.10877665 0.10816674 0.09956369\n",
      "  0.08958935 0.0907682  0.11092582 0.0931933 ]\n",
      " [0.10017168 0.1010754  0.09759578 0.10801288 0.10766491 0.09990894\n",
      "  0.09123871 0.0908081  0.11079033 0.09273327]\n",
      " [0.10059385 0.0999637  0.09779517 0.10905477 0.10684331 0.09983152\n",
      "  0.09106164 0.09216906 0.11013571 0.09255129]\n",
      " [0.10118373 0.09893921 0.09705125 0.10935892 0.10719015 0.10013182\n",
      "  0.09177493 0.09136405 0.11111677 0.09188917]\n",
      " [0.09907249 0.10037204 0.09876016 0.11185459 0.10732404 0.10024963\n",
      "  0.0905678  0.08987439 0.1090348  0.09289007]\n",
      " [0.10163986 0.0993413  0.09780764 0.10758877 0.10800654 0.09946734\n",
      "  0.09029737 0.09084045 0.11235787 0.09265286]\n",
      " [0.10088143 0.09986547 0.09832792 0.10843707 0.10781856 0.10009104\n",
      "  0.09036477 0.09024385 0.1108732  0.0930967 ]\n",
      " [0.10086888 0.10152842 0.09777485 0.10853873 0.1074242  0.10042853\n",
      "  0.08934622 0.09110971 0.11118083 0.09179964]\n",
      " [0.10084598 0.10073949 0.09744915 0.10843025 0.10875405 0.09909352\n",
      "  0.09016751 0.09160218 0.11074639 0.09217147]\n",
      " [0.10044025 0.10069338 0.09741848 0.10899225 0.10849871 0.09986793\n",
      "  0.09017887 0.09049677 0.11194379 0.09146956]\n",
      " [0.10052286 0.10156507 0.09747741 0.10771301 0.10884152 0.10052627\n",
      "  0.09056086 0.09093654 0.11015072 0.09170574]\n",
      " [0.10045549 0.10036953 0.09635355 0.10940278 0.10925205 0.09981904\n",
      "  0.09025244 0.09046785 0.11182226 0.09180503]\n",
      " [0.10081309 0.10086016 0.0976342  0.10832289 0.10685579 0.09973032\n",
      "  0.09031298 0.09156208 0.11124151 0.09266698]\n",
      " [0.10089572 0.10114418 0.09764552 0.10860045 0.10855246 0.09937377\n",
      "  0.0898862  0.090271   0.11206485 0.09156585]\n",
      " [0.10138404 0.1024258  0.09736395 0.10731649 0.10708396 0.09878701\n",
      "  0.09035757 0.09064233 0.11193808 0.09270075]\n",
      " [0.10133403 0.10026293 0.09742331 0.10760592 0.10795844 0.0992406\n",
      "  0.08979228 0.09098685 0.1130127  0.09238294]\n",
      " [0.10086424 0.10149743 0.09775953 0.10885515 0.10688663 0.09937875\n",
      "  0.09104234 0.09156593 0.10952967 0.09262032]\n",
      " [0.1005089  0.10047645 0.09709834 0.10856206 0.10745357 0.09995986\n",
      "  0.09143831 0.09031933 0.11159778 0.0925854 ]\n",
      " [0.10036341 0.10084215 0.09714163 0.1081615  0.1089615  0.10043648\n",
      "  0.08997505 0.09050198 0.11150526 0.09211103]\n",
      " [0.10142888 0.09985085 0.09825165 0.10868717 0.10780128 0.09862307\n",
      "  0.09075489 0.09093039 0.11158061 0.09209119]\n",
      " [0.10098901 0.09913298 0.09752018 0.10885783 0.10698375 0.10021928\n",
      "  0.09080955 0.09117092 0.11238426 0.09193224]\n",
      " [0.10153807 0.10093031 0.09677106 0.10797107 0.10801179 0.09938518\n",
      "  0.08999054 0.09089556 0.11161205 0.09289436]\n",
      " [0.10073089 0.09998209 0.09770486 0.10815283 0.10732251 0.10005456\n",
      "  0.0913244  0.09090121 0.1109218  0.09290484]\n",
      " [0.10156553 0.10033819 0.09666751 0.10819817 0.10763628 0.09932093\n",
      "  0.09129979 0.09103894 0.11124694 0.09268772]\n",
      " [0.10126917 0.09897391 0.09699114 0.10937717 0.10706324 0.10007793\n",
      "  0.09056633 0.09153957 0.11118461 0.09295693]\n",
      " [0.10024949 0.10125049 0.09678582 0.10901869 0.10945415 0.09988352\n",
      "  0.08959392 0.09024538 0.11098586 0.09253267]\n",
      " [0.10105285 0.10093038 0.09755124 0.10796167 0.10836608 0.09960906\n",
      "  0.08977371 0.09090266 0.11217851 0.09167386]\n",
      " [0.10120015 0.10039968 0.0979383  0.10862273 0.10759487 0.10000323\n",
      "  0.08955854 0.09119495 0.11048791 0.09299965]\n",
      " [0.10097497 0.10046804 0.09822449 0.10823047 0.10746106 0.09950314\n",
      "  0.09114385 0.0910644  0.11119734 0.09173225]\n",
      " [0.10037566 0.10109306 0.09751095 0.1092482  0.10840907 0.09900016\n",
      "  0.09104766 0.09171616 0.10919567 0.09240341]\n",
      " [0.10150588 0.10023039 0.09787241 0.10899011 0.10725838 0.10029095\n",
      "  0.089714   0.08952936 0.11183681 0.0927717 ]\n",
      " [0.0998165  0.10137065 0.09883201 0.10936909 0.10737379 0.10000746\n",
      "  0.08960533 0.09191438 0.1098099  0.09190089]\n",
      " [0.10172844 0.1008841  0.09765302 0.10820462 0.10803298 0.09938424\n",
      "  0.08980711 0.09042267 0.11123005 0.09265276]\n",
      " [0.09967542 0.10067311 0.0981348  0.1086642  0.10950295 0.099473\n",
      "  0.08988017 0.09129293 0.11086214 0.09184127]\n",
      " [0.10116345 0.10152401 0.09731329 0.10968796 0.10758929 0.09914324\n",
      "  0.09008238 0.09044638 0.11035156 0.09269843]\n",
      " [0.09902553 0.1016334  0.09803828 0.10918715 0.10890971 0.10013017\n",
      "  0.08996039 0.09076378 0.10996498 0.0923866 ]\n",
      " [0.10111689 0.10054928 0.09826659 0.10871168 0.10852246 0.09978027\n",
      "  0.08955439 0.09081163 0.11092242 0.0917644 ]\n",
      " [0.10109583 0.10058167 0.09796114 0.10697197 0.10717318 0.10028081\n",
      "  0.09141464 0.09206425 0.11058753 0.091869  ]\n",
      " [0.10160123 0.1007139  0.09800603 0.10864322 0.10715008 0.09992257\n",
      "  0.09007044 0.09094959 0.11112294 0.09182001]\n",
      " [0.10135784 0.10009118 0.09716767 0.10759541 0.10788945 0.10071453\n",
      "  0.09094138 0.09091591 0.11106949 0.09225713]\n",
      " [0.10026267 0.10172438 0.09878448 0.1089622  0.10791882 0.10112837\n",
      "  0.08926592 0.09095239 0.10906161 0.09193915]\n",
      " [0.1000898  0.10141908 0.09766985 0.10814486 0.1074651  0.1003387\n",
      "  0.09031851 0.09074249 0.11143004 0.09238157]\n",
      " [0.10187761 0.10074531 0.09806276 0.10817235 0.10621215 0.09914176\n",
      "  0.09044674 0.09274863 0.10982152 0.09277118]\n",
      " [0.10047643 0.10010083 0.09757764 0.10722646 0.10848853 0.09982235\n",
      "  0.09057413 0.0897183  0.11207809 0.09393724]\n",
      " [0.10086099 0.10106264 0.09790689 0.10801804 0.10837511 0.09884571\n",
      "  0.09023217 0.08978998 0.11143136 0.09347712]\n",
      " [0.09989889 0.10134956 0.0979037  0.10688786 0.10798352 0.10013337\n",
      "  0.09010435 0.09119578 0.11167372 0.09286925]\n",
      " [0.10182483 0.1001896  0.09702929 0.1078603  0.10929217 0.09927203\n",
      "  0.09020978 0.09102376 0.11175216 0.09154608]\n",
      " [0.09956282 0.10058493 0.09877999 0.10971256 0.10767294 0.09989645\n",
      "  0.09024011 0.09182196 0.10892646 0.09280179]\n",
      " [0.10107862 0.1008686  0.09716097 0.10893662 0.1093321  0.09894659\n",
      "  0.08972526 0.09078958 0.11015494 0.09300671]\n",
      " [0.10067692 0.10058003 0.09847535 0.10897729 0.10781364 0.09976841\n",
      "  0.09001594 0.09100774 0.11138432 0.09130035]\n",
      " [0.09977825 0.10150705 0.09720898 0.10895686 0.10720484 0.09990515\n",
      "  0.09065401 0.09191947 0.10997817 0.09288723]\n",
      " [0.10158497 0.10067933 0.09712614 0.10848685 0.10694545 0.10036044\n",
      "  0.08943145 0.09070592 0.11230298 0.09237646]\n",
      " [0.09986618 0.10007858 0.09752118 0.10945699 0.10839073 0.09963149\n",
      "  0.0903462  0.09158284 0.11037101 0.0927548 ]\n",
      " [0.10121113 0.10072427 0.09648799 0.10839476 0.10899605 0.09991455\n",
      "  0.09019266 0.09148297 0.10992038 0.09267524]\n",
      " [0.09993554 0.10164066 0.09744669 0.10862329 0.10722653 0.09960939\n",
      "  0.09104161 0.09085038 0.1107414  0.09288451]\n",
      " [0.10100758 0.10054735 0.09745081 0.10881917 0.107216   0.0983922\n",
      "  0.09063987 0.09295139 0.10992792 0.0930477 ]\n",
      " [0.10156504 0.10141298 0.09746431 0.1083467  0.10608787 0.09900541\n",
      "  0.09049414 0.09199408 0.10989001 0.09373946]]\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(100, 784)\n",
    "y = net.predict(x)\n",
    "\n",
    "print(x, y)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-28T02:34:53.386464500Z",
     "start_time": "2025-08-28T02:34:53.218622700Z"
    }
   },
   "id": "94253d6b72342fcf"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100)\n",
      "(100,)\n",
      "(100, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(100, 784)\n",
    "t = np.random.randn(100, 10)\n",
    "\n",
    "grads = net.numerical_gradient(x, t)\n",
    "print(grads['W1'].shape)\n",
    "print(grads['b1'].shape)\n",
    "print(grads['W2'].shape)\n",
    "print(grads['b2'].shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-28T02:38:02.172596Z",
     "start_time": "2025-08-28T02:34:53.386464500Z"
    }
   },
   "id": "5d6efeb879694860"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Implementation of Mini-batch"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4400d365efd29d98"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 23\u001B[39m\n\u001B[32m     20\u001B[39m t_batch = t_train[batch_mask]\n\u001B[32m     22\u001B[39m \u001B[38;5;66;03m# Compute gradient\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m23\u001B[39m grad = network.numerical_gradient(x_batch, t_batch)\n\u001B[32m     25\u001B[39m \u001B[38;5;66;03m# Update parameters\u001B[39;00m\n\u001B[32m     26\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m [\u001B[33m'\u001B[39m\u001B[33mW1\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mb1\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mW2\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mb2\u001B[39m\u001B[33m'\u001B[39m]:\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 51\u001B[39m, in \u001B[36mTwoLayerNet.numerical_gradient\u001B[39m\u001B[34m(self, x, t)\u001B[39m\n\u001B[32m     48\u001B[39m loss_W = \u001B[38;5;28;01mlambda\u001B[39;00m W: \u001B[38;5;28mself\u001B[39m.loss(x, t)\n\u001B[32m     50\u001B[39m grads = {}\n\u001B[32m---> \u001B[39m\u001B[32m51\u001B[39m grads[\u001B[33m'\u001B[39m\u001B[33mW1\u001B[39m\u001B[33m'\u001B[39m] = numerical_gradient(loss_W, \u001B[38;5;28mself\u001B[39m.parms[\u001B[33m'\u001B[39m\u001B[33mW1\u001B[39m\u001B[33m'\u001B[39m])\n\u001B[32m     52\u001B[39m grads[\u001B[33m'\u001B[39m\u001B[33mb1\u001B[39m\u001B[33m'\u001B[39m] = numerical_gradient(loss_W, \u001B[38;5;28mself\u001B[39m.parms[\u001B[33m'\u001B[39m\u001B[33mb1\u001B[39m\u001B[33m'\u001B[39m])\n\u001B[32m     53\u001B[39m grads[\u001B[33m'\u001B[39m\u001B[33mW2\u001B[39m\u001B[33m'\u001B[39m] = numerical_gradient(loss_W, \u001B[38;5;28mself\u001B[39m.parms[\u001B[33m'\u001B[39m\u001B[33mW2\u001B[39m\u001B[33m'\u001B[39m])\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\deep-learning\\Data\\common\\gradient.py:43\u001B[39m, in \u001B[36mnumerical_gradient\u001B[39m\u001B[34m(f, x)\u001B[39m\n\u001B[32m     41\u001B[39m tmp_val = x[idx]\n\u001B[32m     42\u001B[39m x[idx] = \u001B[38;5;28mfloat\u001B[39m(tmp_val) + h\n\u001B[32m---> \u001B[39m\u001B[32m43\u001B[39m fxh1 = f(x)  \u001B[38;5;66;03m# f(x+h)\u001B[39;00m\n\u001B[32m     45\u001B[39m x[idx] = tmp_val - h\n\u001B[32m     46\u001B[39m fxh2 = f(x)  \u001B[38;5;66;03m# f(x-h)\u001B[39;00m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 48\u001B[39m, in \u001B[36mTwoLayerNet.numerical_gradient.<locals>.<lambda>\u001B[39m\u001B[34m(W)\u001B[39m\n\u001B[32m     47\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mnumerical_gradient\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, t):\n\u001B[32m---> \u001B[39m\u001B[32m48\u001B[39m     loss_W = \u001B[38;5;28;01mlambda\u001B[39;00m W: \u001B[38;5;28mself\u001B[39m.loss(x, t)\n\u001B[32m     50\u001B[39m     grads = {}\n\u001B[32m     51\u001B[39m     grads[\u001B[33m'\u001B[39m\u001B[33mW1\u001B[39m\u001B[33m'\u001B[39m] = numerical_gradient(loss_W, \u001B[38;5;28mself\u001B[39m.parms[\u001B[33m'\u001B[39m\u001B[33mW1\u001B[39m\u001B[33m'\u001B[39m])\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 34\u001B[39m, in \u001B[36mTwoLayerNet.loss\u001B[39m\u001B[34m(self, x, t)\u001B[39m\n\u001B[32m     33\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mloss\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, t):\n\u001B[32m---> \u001B[39m\u001B[32m34\u001B[39m     y = \u001B[38;5;28mself\u001B[39m.predict(x)\n\u001B[32m     36\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m cross_entropy_error(y, t)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 25\u001B[39m, in \u001B[36mTwoLayerNet.predict\u001B[39m\u001B[34m(self, x)\u001B[39m\n\u001B[32m     22\u001B[39m W1, W2 = \u001B[38;5;28mself\u001B[39m.parms[\u001B[33m'\u001B[39m\u001B[33mW1\u001B[39m\u001B[33m'\u001B[39m], \u001B[38;5;28mself\u001B[39m.parms[\u001B[33m'\u001B[39m\u001B[33mW2\u001B[39m\u001B[33m'\u001B[39m]\n\u001B[32m     23\u001B[39m b1, b2 = \u001B[38;5;28mself\u001B[39m.parms[\u001B[33m'\u001B[39m\u001B[33mb1\u001B[39m\u001B[33m'\u001B[39m], \u001B[38;5;28mself\u001B[39m.parms[\u001B[33m'\u001B[39m\u001B[33mb2\u001B[39m\u001B[33m'\u001B[39m]\n\u001B[32m---> \u001B[39m\u001B[32m25\u001B[39m a1 = np.dot(x, W1) + b1\n\u001B[32m     26\u001B[39m z1 = sigmoid(a1)\n\u001B[32m     27\u001B[39m a2 = np.dot(z1, W2) + b2\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from Data.dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "train_loss_list = []\n",
    "\n",
    "# Hyperparameters\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # Fetch mini-batch\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # Compute gradient\n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    \n",
    "    # Update parameters\n",
    "    for key in ['W1', 'b1', 'W2', 'b2']:\n",
    "        network.parms[key] -= learning_rate * grad[key]\n",
    "        \n",
    "    # Record the learning process\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-28T03:38:54.576877200Z",
     "start_time": "2025-08-28T02:50:29.516698700Z"
    }
   },
   "id": "b8cf94b8fa938a4f"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 10\u001B[39m\n\u001B[32m      8\u001B[39m test_acc_list = []\n\u001B[32m      9\u001B[39m \u001B[38;5;66;03m# Average repetitions per epoch\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m10\u001B[39m iter_per_epoch = \u001B[38;5;28mmax\u001B[39m(train_size / batch_size, \u001B[32m1\u001B[39m)\n\u001B[32m     12\u001B[39m \u001B[38;5;66;03m# Hyperparameters\u001B[39;00m\n\u001B[32m     13\u001B[39m iters_num = \u001B[32m10000\u001B[39m\n",
      "\u001B[31mNameError\u001B[39m: name 'train_size' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from Data.dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "# Average repetitions per epoch\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "# Hyperparameters\n",
    "iters_num = 10000\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # Fetch mini-batch\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # Compute gradient\n",
    "    grad = net.numerical_gradient(x_batch, t_batch)\n",
    "    \n",
    "    # Update Parameters\n",
    "    for key in ['W1', 'b1', 'W2', 'b2']:\n",
    "        network.parms[key] -= learning_rate * grad[key]\n",
    "        \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    # Compute the accuracy per epoch\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc |\" + str(train_acc) + str(test_acc))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-28T02:38:04.441449400Z",
     "start_time": "2025-08-28T02:38:02.112599300Z"
    }
   },
   "id": "893e868a53a6a04"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
